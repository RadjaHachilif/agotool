[uwsgi]
master-fifo = pytest.fifo

;; attach to zerg
socket = app.sock
zerg = zerg.sock
; Fallback if a zerg server is not available
; By default a Zerg client will not run if the Zerg server is not available. Thus, if your zerg server dies, and you reload the zerg client, it will simply shutdown.
; If you want to avoid that behaviour, add a --socket directive mapping to the required socket (the one that should be managed by the zerg server) and add the --zerg-fallback option.
; With this setup, if a Zerg server is not available, the Zerg client will continue binding normally to the specified socket(s).
zerg-fallback = true

master = true ; false makes sense for debugging with e.g. Valgrind
strict = true ; Fail if unknown config parameter found
enable-threads = false ; Python threads not allowed (don't need them)
vacuum = false ; delete sockets during shutdown. Cleanup after yourself
single-interpreter = true ; single python interpreter per worker instance
die-on-term = true ; Shutdown when receiving SIGTERM
need-app = true ; Fail to start if application cannot load
lazy-apps = true ; loads your application one time per worker. thereby forking is disabled. false in zerg mode, but true with chain-reloading!

;; aGOtool settings
wsgi-file = runserver.py
callable = app
stats = stats_uwsgi.sock
memory-report = true
;; Aquarius /etc/nginx/sites-enabled/agotool.org.conf
auto-procname = true ; give processes names (master and worker)
;; prepend a useful name to the processes (can be seen in htop, but top doesn't show it)
procname-prefix = agotool_pytest_

;; Logging
; disable-logging = true ; uWSGI logging is too verbose
; log-4xx = true ; but we want to see critical errors
; log-5xx = true ; but we want to see critical errors
;; on if not debug
; req-logger = log_uwsgi_requests.log
;; on if not debug
; logger = file:log_uwsgi_error.txt
; To log to files instead of stdout/stderr and to simultaneously daemonize uWSGI.
daemonize = log_vassal.log
safe-pidfile = pid_vassal.pid

;; Buffer size
buffer-size = 32768

;; When to restart workers
max-requests = 80000 ; Restart workers after this many requests (STRING_v11: 80000)
; max-requests-delta = 5000 ; add (worker_id * delta) to the max_requests value of each worker --> for whatever reason setting not recognized
max-worker-lifetime = 604800 ; Restart workers after this many seconds (604800 --> 24*7*60*60, once per week).

;; Dynamic worker scaling
cheaper-algo = busyness
processes = 4 ; Maximum number of workers allowed
cheaper = 1 ; Minimum number of workers allowed
cheaper-initial = 2 ; Workers created at startup
cheaper-overload = 20 ; Length of a cycle in seconds
cheaper-step = 2 ; How many workers to spawn at a time
cheaper-busyness-multiplier = 60 ; How many cycles to wait before killing workers
cheaper-busyness-min = 20 ; Below this threshold, kill workers (if stable for multiplier cycles).
cheaper-busyness-max = 70 ; Above this threshold, spawn new workers
cheaper-busyness-backlog-alert = 16 ; Spawn emergency workers if more than this many requests are waiting in the queue
cheaper-busyness-backlog-step = 2 ; How many emergency workers to create if there are too many requests in the queue
harakiri = 600 ; Number of seconds after which "stuck worker" will get destroyed
